{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-27 16:56:59,457\tINFO resource_spec.py:231 -- Starting Ray with 13.57 GiB memory available for workers and up to 6.81 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-09-27 16:57:00,007\tINFO services.py:1193 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8267\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.7.73',\n",
       " 'raylet_ip_address': '192.168.7.73',\n",
       " 'redis_address': '192.168.7.73:59810',\n",
       " 'object_store_address': '/tmp/ray/session_2020-09-27_16-56-59_456841_47258/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-09-27_16-56-59_456841_47258/sockets/raylet',\n",
       " 'webui_url': 'localhost:8267',\n",
       " 'session_dir': '/tmp/ray/session_2020-09-27_16-56-59_456841_47258'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "# Initialize Ray\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True,num_cpus=8, num_gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object_store_memory': 96.0,\n",
       " 'GPU': 1.0,\n",
       " 'GPUType:GTX': 1.0,\n",
       " 'memory': 278.0,\n",
       " 'CPU': 8.0,\n",
       " 'node:192.168.7.73': 1.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://docs.ray.io/en/latest/tune/user-guide.html#parallelism-gpus\n",
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://localhost:8267\n"
     ]
    }
   ],
   "source": [
    "print(\"Dashboard URL: http://{}\".format(ray.get_webui_url()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MineRL Gym Environment wrapper for action space etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure env_wrappers.py is in the same directory\n",
    "# https://github.com/minerllabs/baselines/blob/master/2019/general/chainerrl/baselines/env_wrappers.py\n",
    "import gym\n",
    "from env_wrappers import (\n",
    "    SerialDiscreteActionWrapper, CombineActionWrapper, SerialDiscreteCombineActionWrapper,\n",
    "    MoveAxisWrapper, FrameSkip, ObtainPoVWrapper, PoVWithCompassAngleWrapper, GrayScaleWrapper\n",
    ")\n",
    "\n",
    "# Sean: Skip logger for now\n",
    "# from logging import getLogger\n",
    "# logger = getLogger(__name__)\n",
    "\n",
    "def wrap_env(env, args, test=False):\n",
    "    # wrap env: time limit...\n",
    "    if isinstance(env, gym.wrappers.TimeLimit):\n",
    "#         logger.info('Detected `gym.wrappers.TimeLimit`! Unwrap it and re-wrap our own time limit.')\n",
    "        env = env.env\n",
    "        max_episode_steps = env.spec.max_episode_steps\n",
    "        \n",
    "        # Sean: ContinuingTimeLimit is a chainrl feature which we will not be using\n",
    "#         env = ContinuingTimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "\n",
    "    # wrap env: observation...\n",
    "    # NOTE: wrapping order matters!\n",
    "\n",
    "    if test and args.monitor:\n",
    "        pass\n",
    "        # Sean: ContinuingTimeLimit is a chainrl feature which we will not be using\n",
    "#         env = ContinuingTimeLimitMonitor(\n",
    "#             env, os.path.join(args.outdir, 'monitor'),\n",
    "#             mode='evaluation' if test else 'training', video_callable=lambda episode_id: True)\n",
    "    if args.frame_skip is not None:\n",
    "        env = FrameSkip(env, skip=args.frame_skip)\n",
    "    if args.gray_scale:\n",
    "        env = GrayScaleWrapper(env, dict_space_key='pov')\n",
    "    if args.env.startswith('MineRLNavigate'):\n",
    "        env = PoVWithCompassAngleWrapper(env)\n",
    "    else:\n",
    "        env = ObtainPoVWrapper(env)\n",
    "    \n",
    "    # Sean: Skip ChainRL requirements\n",
    "#     env = MoveAxisWrapper(env, source=-1, destination=0)  # convert hwc -> chw as Chainer requires.\n",
    "#     env = ScaledFloatFrame(env)\n",
    "    \n",
    "    if args.frame_stack is not None and args.frame_stack > 0:\n",
    "        env = FrameStack(env, args.frame_stack, channel_order='chw')\n",
    "\n",
    "    # wrap env: action...\n",
    "    if not args.disable_action_prior:\n",
    "        env = SerialDiscreteActionWrapper(\n",
    "            env,\n",
    "            always_keys=args.always_keys, reverse_keys=args.reverse_keys, exclude_keys=args.exclude_keys, exclude_noop=args.exclude_noop)\n",
    "    else:\n",
    "        env = CombineActionWrapper(env)\n",
    "        env = SerialDiscreteCombineActionWrapper(env)\n",
    "\n",
    "    # Sean: not supported yet ? according to original file\n",
    "    # env_seed = test_seed if test else train_seed\n",
    "    # env.seed(int(env_seed))  # TODO: not supported yet\n",
    "    return env\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration for MineRL wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agruments for wrapper\n",
    "from datetime import datetime\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Set environment Name\n",
    "        self.env = 'MineRLNavigateDense-v0' #'MineRLNavigateDenseVectorObf-v0'\n",
    "        \n",
    "        # Set frame skipping or stacking\n",
    "        self.frame_skip = None\n",
    "        self.frame_stack = None\n",
    "        \n",
    "        # Set gray scale or rgb input\n",
    "        self.gray_scale = False\n",
    "        \n",
    "        # Toggle for monitoring / video recordings\n",
    "        self.monitor = False\n",
    "        \n",
    "        # Output folder for monitor\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%m_%d_%Y_HR_%H_MIN_%M_SEC_%S\")\n",
    "        self.outdir = f'results/{self.env}/date_time'\n",
    "        \n",
    "        # Disable prior action sets such as repeating or excluding actions\n",
    "        self.disable_action_prior = True\n",
    "        \n",
    "        # Set always pressed / repeated keys by agent\n",
    "        self.always_keys = 'forward sprint attack' # Just an example !\n",
    "        \n",
    "        # Set excluded keys for agent\n",
    "        exclude_keys = 'back left right sneak place' # Just an example !\n",
    "        \n",
    "        # Check code from baseline script for more info:\n",
    "        # https://github.com/minerllabs/baselines/blob/master/2019/general/chainerrl/baselines/ppo.sh\n",
    "        # https://github.com/minerllabs/baselines/blob/master/2019/general/chainerrl/baselines/ppo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering MineRL Envs in RLLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "def minerl_env_creator(env_config):\n",
    "    import minerl # Load MineRL environments as Gym Environments\n",
    "#     import gym\n",
    "    # Load wrapper configurations\n",
    "    args = Args()\n",
    "    \n",
    "    # Create Minecraft environment\n",
    "    core_env = gym.make(args.env)\n",
    "    minerl_env = wrap_env(core_env, args, test=False)\n",
    "    \n",
    "    # Code below is from original code in \n",
    "    # https://github.com/minerllabs/baselines/blob/master/2019/general/chainerrl/baselines/ppo.py\n",
    "    # eval_env = gym.make(args.env)  # Can't create multiple MineRL envs\n",
    "    # eval_env = wrap_env(eval_env, test=True)\n",
    "    # eval_env = wrap_env(core_env, test=True)\n",
    "    \n",
    "    return minerl_env  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register MineRL environment in RLLIB\n",
    "register_env(\"minerl\", minerl_env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import minerl\n",
    "# import gym\n",
    "# # Check minerl environments\n",
    "# # This only checks Gym environment not the ones registered in Ray!\n",
    "# all_envs = gym.envs.registry.all()\n",
    "# env_ids = [env_spec.id for env_spec in all_envs]\n",
    "# print(env_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test run\n",
    "\n",
    "# import minerl\n",
    "# args = Args()\n",
    "# core_env = gym.make(args.env)\n",
    "# minerl_env = wrap_env(core_env, args, test=False)\n",
    "# minerl_env.reset()\n",
    "# for i in range(500):\n",
    "#     minerl_env.step(minerl_env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register custom vision network to process input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/blackbox/anaconda3/envs/minerl/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# Registering a custom model, simple version --> fully connected network\n",
    "# We need this because the default configurations for the model network is\n",
    "# not compatible with the shape of the output from the MineRL environment\n",
    "# The output is a 64 x 64 pixels with RGBA of game play P.O.V. --> shape = (4, 64, 64)\n",
    "\n",
    "# Otherwise we will get:\n",
    "# ValueError: No default configuration for obs shape [4, 64, 64], you must specify \n",
    "# `conv_filters` manually as a model option. Default configurations are only available\n",
    "# for inputs of shape [42, 42, K] and [84, 84, K]. You may alternatively want to use \n",
    "# a custom model or preprocessor.\n",
    "\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "\n",
    "# https://docs.ray.io/en/releases-0.8.5/rllib-examples.html\n",
    "# The register custom env and model links to custom_env.py\n",
    "# https://github.com/ray-project/ray/blob/master/rllib/examples/custom_env.py\n",
    "class TorchCustomModel(TorchModelV2, nn.Module):\n",
    "    \"\"\"Example of a PyTorch custom model that just delegates to a fc-net.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.torch_sub_model = TorchFC(obs_space, action_space, num_outputs,\n",
    "                                       model_config, name)\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        input_dict[\"obs\"] = input_dict[\"obs\"].float()\n",
    "        fc_out, _ = self.torch_sub_model(input_dict, state, seq_lens)\n",
    "        return fc_out, []\n",
    "\n",
    "    def value_function(self):\n",
    "        return torch.reshape(self.torch_sub_model.value_function(), [-1])\n",
    "\n",
    "ModelCatalog.register_custom_model(\"fc_pov\", TorchCustomModel)\n",
    "# Weird tensorflow error? even though using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://groups.google.com/forum/#!topic/ray-dev/coaz8dgHyYw\n",
    "# https://docs.ray.io/en/latest/rllib-training.html#specifying-resources\n",
    "# Setting resources_per_trial={\"cpu\": 8, \"gpu\": 1} will cause issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/31.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.57 GiB heap, 0.0/4.69 GiB objects (0/1.0 GPUType:GTX)<br>Result logdir: /home/blackbox/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_minerl_24b7b_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m WARNING:tensorflow:From /home/blackbox/anaconda3/envs/minerl/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m 2020-09-27 16:57:06,883\tWARNING deprecation.py:30 -- DeprecationWarning: `use_pytorch` has been deprecated. Use `framework=torch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m 2020-09-27 16:57:06,883\tWARNING deprecation.py:30 -- DeprecationWarning: `eager` has been deprecated. Use `framework=tfe` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m 2020-09-27 16:57:06,883\tINFO trainer.py:632 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m /home/blackbox/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m 2020-09-27 16:57:44,736\tINFO trainable.py:251 -- Trainable.setup took 38.238 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m 2020-09-27 16:57:44,736\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=47391)\u001b[0m WARNING:tensorflow:From /home/blackbox/anaconda3/envs/minerl/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=47391)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=47391)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=47391)\u001b[0m /home/blackbox/anaconda3/envs/minerl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=47391)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=47391)\u001b[0m /home/blackbox/anaconda3/envs/minerl/lib/python3.7/site-packages/ray/rllib/utils/torch_ops.py:149: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[2m\u001b[36m(pid=47391)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_minerl_24b7b_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-09-27_16-59-32\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 5c00d60eb8004d97a567ce40e6a53f34\n",
      "  experiment_tag: '0'\n",
      "  hostname: blackbox\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.2\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 2.45800544321537\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027558082132600248\n",
      "        policy_loss: -0.010829467937583104\n",
      "        total_loss: 0.2658954306971282\n",
      "        vf_explained_var: 0.11666689068078995\n",
      "        vf_loss: 0.2712132791057229\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.7.73\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.34220779220779\n",
      "    ram_util_percent: 63.077272727272735\n",
      "  pid: 47392\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 107.59872436523438\n",
      "  time_this_iter_s: 107.59872436523438\n",
      "  time_total_s: 107.59872436523438\n",
      "  timers:\n",
      "    learn_throughput: 261.3\n",
      "    learn_time_ms: 15308.052\n",
      "    sample_throughput: 43.355\n",
      "    sample_time_ms: 92261.702\n",
      "    update_time_ms: 17.321\n",
      "  timestamp: 1601251172\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 24b7b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/31.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.57 GiB heap, 0.0/4.69 GiB objects (0/1.0 GPUType:GTX)<br>Result logdir: /home/blackbox/ray_results/PPO<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_minerl_24b7b_00000</td><td>RUNNING </td><td>192.168.7.73:47392</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         107.599</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">     nan</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_minerl_24b7b_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-09-27_17-00-46\n",
      "  done: true\n",
      "  episode_len_mean: 6000.0\n",
      "  episode_reward_max: 8.999691009521484\n",
      "  episode_reward_mean: 8.999691009521484\n",
      "  episode_reward_min: 8.999691009521484\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: 5c00d60eb8004d97a567ce40e6a53f34\n",
      "  experiment_tag: '0'\n",
      "  hostname: blackbox\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.30000000000000004\n",
      "        cur_lr: 5.0e-05\n",
      "        entropy: 2.4419078305363655\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01417196518741548\n",
      "        policy_loss: -0.02255858271382749\n",
      "        total_loss: 0.14833057206124067\n",
      "        vf_explained_var: 0.2861601710319519\n",
      "        vf_loss: 0.166637564310804\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.7.73\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.852380952380955\n",
      "    ram_util_percent: 69.03333333333335\n",
      "  pid: 47392\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 7.244723779740087\n",
      "    mean_inference_ms: 4.238673410271424\n",
      "    mean_processing_ms: 2.743244409531359\n",
      "  time_since_restore: 181.44350290298462\n",
      "  time_this_iter_s: 73.84477853775024\n",
      "  time_total_s: 181.44350290298462\n",
      "  timers:\n",
      "    learn_throughput: 275.293\n",
      "    learn_time_ms: 14529.996\n",
      "    sample_throughput: 52.518\n",
      "    sample_time_ms: 76164.856\n",
      "    update_time_ms: 14.663\n",
      "  timestamp: 1601251246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 24b7b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.7/31.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/13.57 GiB heap, 0.0/4.69 GiB objects (0/1.0 GPUType:GTX)<br>Result logdir: /home/blackbox/ray_results/PPO<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_minerl_24b7b_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         181.444</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> 8.99969</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.7/31.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/13.57 GiB heap, 0.0/4.69 GiB objects (0/1.0 GPUType:GTX)<br>Result logdir: /home/blackbox/ray_results/PPO<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_minerl_24b7b_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         181.444</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> 8.99969</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m *** Aborted at 1601251249 (unix time) try \"date -d @1601251249\" if you are using GNU date ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fb7fda97ed0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m PC: @                0x0 (unknown)\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m *** SIGSEGV (@0x7fc1eee1c9d0) received by PID 47566 (TID 0x7fc3cd309740) from PID 18446744073422358992; stack trace: ***\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x7fc3cd67a3c0 (unknown)\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x7fc3cd66faab __pthread_clockjoin_ex\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x7fc3cb2452d3 std::thread::join()\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x7fc3cb776493 ray::gcs::GlobalStateAccessor::Disconnect()\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x7fc3cb615fbc __pyx_pw_3ray_7_raylet_19GlobalStateAccessor_5disconnect()\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e341b71 _PyMethodDef_RawFastCallKeywords\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e348aef _PyMethodDescr_FastCallKeywords\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e3ad37c _PyEval_EvalFrameDefault\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e34120b _PyFunction_FastCallKeywords\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e3a8e70 _PyEval_EvalFrameDefault\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e2f12b9 _PyEval_EvalCodeWithName\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e341435 _PyFunction_FastCallKeywords\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e3a8be6 _PyEval_EvalFrameDefault\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e2f12b9 _PyEval_EvalCodeWithName\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e2f23e5 _PyFunction_FastCallDict\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e356c70 PyErr_CheckSignals\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e3629c6 time_sleep\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e341abd _PyMethodDef_RawFastCallKeywords\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e341db1 _PyCFunction_FastCallKeywords\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e3ad5be _PyEval_EvalFrameDefault\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e2f1b00 _PyEval_EvalCodeWithName\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e2f23e5 _PyFunction_FastCallDict\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e3aa51a _PyEval_EvalFrameDefault\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e34120b _PyFunction_FastCallKeywords\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e3a8e70 _PyEval_EvalFrameDefault\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e34120b _PyFunction_FastCallKeywords\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e3a8e70 _PyEval_EvalFrameDefault\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e34120b _PyFunction_FastCallKeywords\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e3a8e70 _PyEval_EvalFrameDefault\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e2f231b _PyFunction_FastCallDict\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e310b93 _PyObject_Call_Prepend\n",
      "\u001b[2m\u001b[36m(pid=47392)\u001b[0m     @     0x55ac1e3480aa slot_tp_init\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0927 17:00:49.377543 47338 47338 process.cc:434] Failed to kill process 47391 with error system:3: No such process\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "# https://docs.ray.io/en/latest/rllib-training.html#tuned-examples\n",
    "\n",
    "tune.run(\"PPO\",\n",
    "         config={\"env\": \"minerl\",\n",
    "                 \"num_gpus\": 1,\n",
    "                 \"num_workers\": 1,\n",
    "                 \"num_envs_per_worker\": 1,\n",
    "                 \"eager\": False,\n",
    "                 \"use_pytorch\": True,\n",
    "#                  'monitor':True, \n",
    "                 \"model\": {\n",
    "                    \"custom_model\": \"fc_pov\",\n",
    "                    }\n",
    "                 }\n",
    "         ,stop={\"training_iteration\": 2})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Something weird about using Trainer + Tune\n",
    "# from ray import tune\n",
    "# import ray.rllib.agents.ppo as ppo\n",
    "# # from ray.rllib.agents.dqn import PPOTrainer\n",
    "# # from ray.rllib.agents.dqn import DQNTrainer\n",
    "\n",
    "# # https://docs.ray.io/en/latest/rllib-training.html#tuned-examples\n",
    "# config = ppo.DEFAULT_CONFIG.copy()\n",
    "# config[\"num_gpus\"] = 1\n",
    "# config[\"num_workers\"] = 1\n",
    "# config[\"num_envs_per_worker\"] = 1\n",
    "# config[\"eager\"] = False\n",
    "# trainer = ppo.PPOTrainer(config=config)#, env=\"minerl\")\n",
    "\n",
    "# tune.run(trainer,\n",
    "#          config={\"env\": \"minerl\",\n",
    "#                  \"use_pytorch\": True,\n",
    "# #                  'monitor':True, \n",
    "#                  \"model\": {\n",
    "#                     \"custom_model\": \"fc_pov\",\n",
    "#                     }\n",
    "#                  }\n",
    "#          ,stop={\"training_iteration\": 2},\n",
    "#         resources_per_trial={\"cpu\": 8, \"gpu\": 1})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
